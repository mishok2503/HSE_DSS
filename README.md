# Key value store

**Архитектура**

Главные компоненты: `Filter (Bloom's), Log HashTable (open addressing, Robin Hood hashing, on DRAM) + Log file(SSD)`, `Global HashTable (closed addressing, bucket is memory page, on SSD)`,
`HoT file (hot values and keys)`, `Data file (all values)`.

Как примерно всё устроено: при добавлении мы так или иначе записываем значение на Log file (если там хранилище переполняется, то мы копируем это дело в `Data file`). А что происходит с ключами? Ключи сначала попадают на первый уровень, в `Log HashTable`, там при использовании хеширования Робина Гуда получается добиться высокого коэффицента заполненности при сохранении очень быстрого `lookup`. При обновлении мы можем посмотреть в LOG и очень быстро перезаписать значение по уже указанному offset'у, что тоже весьма удобно. При удалении всё тоже весьма нехитро: если значение есть в LOG, то удаление будет выглядеть крайне просто, достаточно пометить соответствующий элемент в `Log file` как удалённый и удалить элемент из `Log HashTable`. Если же нам не повезло, то надо будет пойти в большую таблицу на `SSD` (`Global HashTable`) и поискать ключ там (это делается весьма просто, поскольку достаточно посчитать у ключа хеш и прочитать выделенную заранее под бакет страницу и за линию сравнить с присутствующими там ключами, что эффективно с точки зрения числа чтений с диска, всего одно). Если ключ нашёлся, то смотрим, какой offset у него записан. В теории там может быть два значения, но при наличии величины offset'а в `Hot file` искать будем именно там, поскольку операция чтения с `SSD` намного быстрее, чем с `HDD` и к тому же на HDD может храниться неактуальная версия. C получением нового ключа должно быть несколько проще, поскольку

Хотим посчитать - кол-во шардов, отношение ssd/hdd, как часто пересчитывать фильтр, параметры мап. как часто умирает ssd


Заметки:  
  Наша основная цель -- перфоманс  
  Поэтому нет сжатия, поэтому не очень хорошая отказоустойчивость  
  Для каждого шарда храним свой фильтр(идеальная парралельность).  
  Для лучшего распределения ключей, будем шардить по их хэшам ключей.  
  Размер данных ~ размер страницы => пишем сначала в удалённые  
  фильтр - блум, так как он по перфомансу быстрее + мало памяти  
  балансировка шардов - если максимум отклонений от среднего слишком большой - перестраиваем вообще все шарды(полная перестройка)  
  В SSD - "горячие" файлы
